\section{Motivation}\label{section:motivation}

\lettrine{D}{istributed} computing is naturally a field that explores non-trivial levels of parallelism on a large
scale, and as such, any fledgling distributed system demands a testbed that epitomizes these properties. It then
follows that network simulation is likely to be the apparatus of choice, notwithstanding the possibility of expending
serious resource to conduct ``live'' tests over an existing physical topology, perhaps even the internet itself.

In spite of this, designing and building a simulation tool is an inherently multivariate and sophisticated undertaking
in its own right. A packet can experience a great deal during its lifetime. Indeed, most packets are ultimately
buffered, serialized into bits, sent over a wire and are then thrust into a hellish domain whereby they can be
subject to the horrors of corruption, duplication, reordering, delay and possibly even outright elimination. To boot,
they can sometimes go airborne during this process, having been shot out of a wireless router, only to land right
back into the terror-laden realm that we so innocently call ``the internet''.

It is in this way that good simulation software should be capable of accurately mimicking the these behaviours
according to any arbitrary configuration. In fact, the potential for accute granularity that a simulator offers makes
it far superior to any ``real'' test scenario, insofar as it can be moulded to investigate specific nuances within a
particular algorithm or protocol. For example, most low-level distributed technologies must find ways to overcome
both packet loss and corruption. Although these are independant issues, they tend to co-exist in practice and are
often correlated phenomena, which makes isolated testing difficult. This problem is absent in a simulated context
however, since virtual environments can be tuned to boast whatever characteristics are deemed necessary for
the given exercise. In resolution of the aforementioned example, two simulations can be run at the descretion of the
developers, one where corruption is the only danger present, along with a similar configuration that drops packets
instead.

Another clear advantage of simulated methods is their scalability; an advantage that becomes especially lucrative for
any accounting department. A simulation's scope is typically bounded by the capacity and processing capabilities of
the machine on which it runs. Even if a machine was only able to emulate two nodes effectively, then this still
delivers the possibility of a two-fold cost saving. Nonetheless, most high-end commericial computer systems should be
capable of simulating topologies containing hundreds if not \emph{thousands} of nodes, given a sufficiently
intelligent framework. It could well be the case that much of the virtual space simulation occupies is untouchable by
any real-world network; that is to say that simulated techniques could explore strictly hypothetical territory,
furnishing insights into distributed algorithms that reality never could.


\section{Objectives}\label{section:objectives}

\lettrine{I}{f} the ultimate goal of this project were to be expressed as a succinct yet all-encompassing sound-bite,
it would probably be \emph{to provide those with a curiosity for distributed computer systems with the means to tinker
around with network technologies as though they were running in the real world.} Indeed, whilst this may capture the
spirit of what is being worked towards, it is very abstract and unspecific; in order to distill this
mission-statement into a set of more tangible objectives, it is probably best framed with respect to what users can
expect from the end product, i.e.:

\begin{enumerate}
    \item An interface to design an arbitrary network topology.
    \item A configuration suite to define how packets are manipulated during message-passing.
    \item A mechanism to run a distributed algorithm across a virtual network topology which reflects the properties
    described by the user as per objectives 1) and 2).
    \item Platform agnosticism:
    \begin{enumerate}
        \item Basic usage of the tool should not depend on the operating system or hardware being used.
        \item Users should not be pigeonholed into working with a particular programming language in order to
        simulate their solution.
    \end{enumerate}
    \item Plug-and-playability:
    \begin{enumerate}
        \item Users should need minimal domain-specific knowledge in order to use the full set of features on offer.
        \item The prospective tool should be able to mimic a real network in a way that minimises bespoke set-up,
        i.e.: if a user normally tests their distributed algorithm using real computers connected over a physical
        network, then transitioning to using the prospective tool should be more or less seamless.
    \end{enumerate}
\end{enumerate}

Not only do these objectives provide the project with a selection of qualitative yet precise aims, they also imbue
the development process with a sense of direction and purpose. The picture being painted of the end user becomes more
refined and vivid through the act of clarifying their needs and how they can be met. In this instance, the key
qualities that would hopefully appeal to prospective users are versatility and ease-of-use. One might conceptualise
the proposed solution as a black box that appears from the outside to contain an entire network topology, whereby
users can adjust its shape and behaviour using an intuitive control-panel of dials and knobs. In turn, students,
developers and enthusiasts alike can probe this black box to achieve whatever network-related ends they see fit, with
minimal configuration and bespoke setup required.


\section{Contributions}\label{section:contributions}

TODO
