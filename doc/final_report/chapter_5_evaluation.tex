\section{Unit Testing}\label{section:unit_testing}

Whilst unit testing is often seen to be a menial aspect of ensuring robustness in software engineering projects,
unit tests in Packet Courier are used to verify the correctness of the statistical distribution implementations,
which is somewhat non-trivial. Each distribution is subjected to two different tests:
\begin{itemize}
    \item $n$ samples are taken of the distribution variable $X$ and their mean $\bar{X}$ must lie within an $\alpha
    = 0.01$ confidence interval. $n$ is set to $10,000$ for this test. This is exactly why the \texttt{mean} and
    \texttt{variance} methods in \texttt{Distribution<T>} are necessary, as per the end of
    Section~\ref{subsection:statistical_distribution_api}. The benefit of this test is that it assures statistical
    distribution API users that the distributions they are using are unbiased and boast the correct expected value.
    \item $n$ samples are taken of the distribution variable $X$ and separated into $m$ contiguous bins, $B_1, \dots ,
    B_m$ delimited by boundary values $b_1, \dots , b_m, b_{m+1}$. The contents of each bin $B_i$ must reflect the
    following property: $\frac{|B_i|}{m} - \delta \leq F_X(b_{i+1}) - F_X(b_i) \leq \frac{|B_i|}{m} + \delta$ for
    $\delta = 0.01$. $n$ is set to $50,000$ for this test. The benefit of this test is that is assures statistical
    distribution API users that the distributions have roughly the correct shape. In this way, one could think about
    this test like the proof of calculus' Trapezoidal Rule\cite{trapezoidal_rule}, whereby the trapezoids under the
    curve are analogous to the bins under the probability density function.
\end{itemize}

This is technically achieved by having a base abstract \texttt{DistributionTest<T>} class that implements tests
generically using the \texttt{Distribution<T>} interface. The key abstract method is
\texttt{getSomeDistributionsWithCdfTables} which generates a selection of distributions and tabulated cumulative
distribution functions. In turn, a suite of test classes inherit from \texttt{DistributionTest<T>} and supply it with
some distributions and CDF-tables to conduct tests on.


\section{System Testing}\label{section:system_testing}

Packet Courier takes advantage of system testing to ensure that the aggregate properties of packets sent over an
emulated network accurately reflect those specified in the \texttt{.courierconfig} file. A selection of
configurations have been specified in \texttt{src/test/resources/thorpe/luke/network/simulation/analysis} to cover
the six fundamental offerings: corruption, drop, duplication, latency, limit and throttle. These can be run from the
root of the directory using \texttt{run\_basic\_analysis\_suite.sh}. Each configuration consists of a star topology
with either 5, 25, 50, 75 or 100 clients which send 50 packets per second to a server. Clients log the contents of
their packet and when they sent them. The server logs the contents of the packet it has received and when it received
them. The logs are then dumped to a file and analysed in post.

\subsection{Analysis Methodology}\label{subsection:analysis_methodology}

Each client process runs the \texttt{analysis\_client.py} script which has been designed to send packets that
encapsulate all the information required to assess any transformations they may have undergone during transit. This
information includes:
\begin{itemize}
    \item A special prefix, \texttt{!} so that it is clear from the logs that this is a client send.
    \item The name of the client.
    \item The ordinality of the packet, i.e.: was it the 1\textsuperscript{st}, 4\textsuperscript{th} or
    17\textsuperscript{th} packet sent by that particular client?
    \item The date and time of sending.
    \item A checksum of the above information using the SHA-256 cryptographic hash\cite{sha256_hash,
        python_sha256_hash}.
\end{itemize}

Each of the above properties are delimited using the \texttt{~} symbol and encoded into bytes using the UTF-8
standard\cite{utf8}, the result of which tends to take up between 132 and 133 bytes, so the datagram buffer size is
set to 136 to prevent truncation. Clients log the contents of each packet they send to the server.

The server running \texttt{analysis\_server.py} will listen for packets and log the date and time of receipt
immediately before attempting to parse the packet for its contents. If the packet cannot be decoded using UTF-8 or
does not have the correct number of elements after having been split based on the delimiter \texttt{~}, then the
packet will be logged as \texttt{Junk!} Otherwise the elements of the packet will be hashed and compared with the
checksum to check for corruption. The server will then log the following contents:
\begin{itemize}
    \item A special prefix, \texttt{?}, so that it is clear from the logs that this is a server receipt.
    \item The name of the server.
    \item The date and time of receipt.
    \item The name of the client.
    \item The ordinality of the packet, i.e.: was it the 1\textsuperscript{st}, 4\textsuperscript{th} or
    17\textsuperscript{th} packet sent by that particular client?
    \item The date and time of sending.
    \item A \texttt{True} if the packet had been corrupted according to the checksum; \texttt{False} otherwise.
\end{itemize}

Note that a sent packet can be uniquely identified by the sending client's name and its ordinal number: this is
referred to as the packet-id. As such, the log files containing this information can be used to aggregate the
following statistics:
\begin{itemize}
    \item \textbf{The duration of the analysis run.} \\
    \emph{The last uncorrupted receipt minus the first uncorrupted send.}
    \item \textbf{The number of packets sent by clients.}
    \item \textbf{The number of packets received by the server.}
    \item \textbf{The number of unexpected arrivals.} \\
    \emph{How many times the server receives a packet-id that it has already seen.}
    \item \textbf{The number of missing arrivals.} \\
    \emph{How many packets that were sent by a client but couldn't be reliably identified by the server. This could
    either be because they were dropped or because they were junk upon arrival.}
    \item \textbf{The number of unidentifiable arrivals.} \\
    \emph{How many times the server receives a packet-id that was never sent by any of the clients.}
    \item \textbf{The number of checksum corrupted packets.}
    \item \textbf{The number of junk packets.}
\end{itemize}

These key metrics can be used to verify the correctness and robustness of each core Packet Courier functionality.


\section{User Feedback}\label{section:user_feedback}

TODO


\section{Analysis of Objectives}\label{section:analysis_of_objectives}

\subsection{Network Topology Design Interface}\label{subsection:network_topology_design_interface}

\textbf{Objective 1: }\emph{An interface to design an arbitrary network topology.}

\subsection{Network Conditions Configuration Suite}\label{subsection:network_conditions_configuration_suite }

\textbf{Objective 2: }\emph{A configuration suite to define how packets are manipulated during message-passing.}

\subsection{Simulation of a Distributed Algorithm}\label{subsection:simulation_of_a_distributed_algorithm}

\textbf{Objective 3: }\emph{A mechanism to run a distributed algorithm across a virtual network topology which
reflects the
properties described by the user as per objectives 1) and 2).}

\subsection{Platform Agnosticism}\label{subsection:platform_agnosticism}

\textbf{Objective 4.a: }\emph{Basic usage of the tool should not depend on the operating system or hardware being used
.} \\ \\
\textbf{Objective 4.b: }\emph{Users should not be pigeonholed into working with a particular programming language in
order to
simulate their solution.}

\subsection{Plug-and-playability}\label{subsection:plug_and_playability}

\textbf{Objective 5.a: }\emph{Users should need minimal domain-specific knowledge in order to use the full set of
features on offer.} \\ \\
\textbf{Objective 5.b: }\emph{The prospective tool should be able to mimic a real network in a way that minimises
bespoke set-up, i.e.: if a user normally tests their distributed algorithm using real computers connected over a
physical network, then transitioning to using the prospective tool should be more or less seamless.}
